{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e7f6fd",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "\n",
    "KARL MAKE SURE TO UPDATE AND REFERENCE \n",
    "\n",
    "### Goal:\n",
    "An MNIST image is 784 pixels when unravelled. It is B/W, with values from 0-255. I want to create a ML model which will recognise the number written in the image.\n",
    "\n",
    "### To Investigate:\n",
    "How changing the number of hidden layer nodes affects the output. \n",
    "\n",
    "How adding layers or changing the nonlinear relationship (activation fn) between the layers affects the output. tanh, sigmoid, relu\n",
    "\n",
    "Graphing how quickly the model learns and reaches a threshold. \n",
    "\n",
    "How to squeeze a very accurate model out of it, maybe combine them? \n",
    "\n",
    "something with the back prop?\n",
    "\n",
    "how does the alpha affect it? \n",
    "\n",
    "instead of iterations, until an accuracy is reached? \n",
    "\n",
    "should I add some randomness to try find a global min?\n",
    "\n",
    "trying a different descent - with momentum? \n",
    "\n",
    "### How it works\n",
    "Need to transpose first for Matrix Multiplication\n",
    "#### Forward Propagation\n",
    "This is how an image will proceed through the network\n",
    "\n",
    "A[0] = input layer (784 nodes)\n",
    "\n",
    "A[1] = ReLU( w[1]*A[0]+b[1] ), ReLU is the activation fn, it makes it a nonlinear relationship, RELU is f(x) = x, x>0, =0 x<=0. w is weights and b is biases. (10 nodes)\n",
    "\n",
    "A[2] = g(w[2]*A[1] +b[2]), g is softmax as this goes to the output layer. softmax is essentially exp() / sum(exp()). gives a probability. (10 nodes, 0-1 probability of being the value 0-9)\n",
    "\n",
    "#### Backward Propagation\n",
    "The thing that makes it learn. Want to see how wrong the model was and adjust the weights and biases\n",
    "\n",
    "what I want to know is how we know how much to change the weights by? \n",
    "\n",
    "this is the part I least understand\n",
    "\n",
    "obviously you have to do it in reverse - so the transpose of the matrix and the differential of the activation fn. \n",
    "\n",
    "dz[2] = A[2] - Y, Y is the answer from the catalogue\n",
    "\n",
    "unsure about this...\n",
    "\n",
    "dw[2] = 1/m dz[2]* a[1]^T, derivative of the loss fn wrt the weights, m is no of columns (ie number of images as it's been transposed)\n",
    "\n",
    "db[2] = 1/m sum dz[2], average of the absolute error \n",
    "\n",
    "dz[1] = w[2]^T * dz[2] * g'(z[1]), 1st derivative of activation fn \n",
    "\n",
    "dw[1] = 1/m dz[1] * A[0]^T\n",
    "\n",
    "db[1] = 1/m sum(dz[1])\n",
    "\n",
    "#### Update parameters\n",
    "need to choose \\alpha, learning rate hyperparameterm\n",
    "\n",
    "for each parameter (eg w[2]) you remove the delta times learning rate (-\\alpha* dw[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6bb40",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f277ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "#get data\n",
    "data = pd.read_csv('train.csv')\n",
    "data = np.array(data) #convert to numpy array \n",
    "\n",
    "m,n = data.shape #rows and columns m&n\n",
    "np.random.shuffle(data) #shuffle\n",
    "\n",
    "data = data.T #transpose \n",
    "\n",
    "labels = data[0] \n",
    "X = data[1:]/255 #these will be the imputs, /255 to normalise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e87416",
   "metadata": {},
   "source": [
    "data is now nxm, ie 785 x 42000 array. the extra row is the label of 0-9\n",
    "\n",
    "#### Generate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "900b1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RELU(a):\n",
    "    \"\"\"\n",
    "    Performs the ReLU function, if the value is above zero it is returned, if it is below zero, zero is returned\n",
    "    \"\"\"\n",
    "    return np.maximum(a,0)\n",
    "\n",
    "def softmax(a):\n",
    "    \"\"\"\n",
    "    Performs softmax, essentially will get a probability out\n",
    "    \"\"\"\n",
    "    ####Not sure about this one \n",
    "    return np.exp(a) / (sum(np.exp(a)))\n",
    "\n",
    "def RELU_prime(a):\n",
    "    \"\"\"\n",
    "    The derivative of the RELU fn\n",
    "    \"\"\"\n",
    "    \n",
    "    return a>0 \n",
    "\n",
    "    #############again unsure - different? \n",
    "\n",
    "def initialise():\n",
    "    \"\"\"\n",
    "    Randomly initialises the weights and biases\n",
    "    \"\"\"\n",
    "    w1 = np.random.normal(size = (10, 784))\n",
    "    b1 = np.random.normal(size = (10, 1))\n",
    "    w2 = np.random.normal(size = (10, 10))\n",
    "    b2 = np.random.normal(size = (10, 1))\n",
    "    \n",
    "    return w1,b1,w2,b2\n",
    "\n",
    "\n",
    "def forwardprop(w1,b1, w2,b2, x):\n",
    "    \"\"\"\n",
    "    Forward propagates the image through the net\n",
    "    \"\"\"\n",
    "    z1 = w1.dot(x) + b1 #apply weights and biases\n",
    "    a1 = RELU(z1) #activate layer\n",
    "    \n",
    "    z2 = w2.dot(a1) + b2 #apply w&b for 2nd layer\n",
    "    a2 = softmax(z2) #activate for output layer\n",
    "    \n",
    "    return z1,a1,z2,a2\n",
    "\n",
    "def fix(y):\n",
    "    Y = np.zeros((y.size, y.max() + 1))\n",
    "    Y[np.arange(y.size), y] = 1\n",
    "    return Y.T\n",
    "\n",
    "def backprop(z1,a1, z2,a2, w1,w2, x,y): #y is labels \n",
    "    global m \n",
    "    #need to get y in a nice format before putting it in here\n",
    "    y = fix(y)\n",
    "    \n",
    "    dz2 = a2 - y\n",
    "    dw2 = 1/m * dz2.dot(a1.T)\n",
    "    db2 = 1/m * np.sum(dz2)\n",
    "    \n",
    "    dz1 = (w2.T).dot(dz2) * RELU_prime(z1)\n",
    "    dw1 = 1/m * dz1.dot(x.T)\n",
    "    db1 = 1/m * np.sum(dz1)\n",
    "    \n",
    "    return dw1, db1,dw2,db2\n",
    "\n",
    "def update(w1,b1, w2,b2, dw1,db1, dw2,db2, alpha):\n",
    "    w1 = w1 - alpha * dw1\n",
    "    b1 = b1 - alpha * db1\n",
    "    w2 = w2 - alpha * dw2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return w1,b1, w2,b2\n",
    "        \n",
    "def accuracy(prediction,y):\n",
    "    return np.sum(prediction==y)/y.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5f01a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LEARN(x,y, iterations, alpha=0.7):\n",
    "    w1,b1,w2,b2 = initialise()\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        z1,a1,z2,a2 = forwardprop(w1,b1,w2,b2, x)\n",
    "        \n",
    "        dw1,db1, dw2,db2 = backprop(z1,a1, z2,a2, w1,w2, x,y)\n",
    "        \n",
    "        w1,b1, w2,b2 = update(w1,b1, w2,b2, dw1,db1, dw2,db2, alpha)\n",
    "        \n",
    "        if i % 50 ==0:\n",
    "            print(i)\n",
    "#             print('a2', a2.shape)\n",
    "            prediction = np.argmax(a2,axis=0)\n",
    "            print(prediction.shape)\n",
    "            print(prediction)\n",
    "#             print(\"pred\",prediction.shape)\n",
    "#             print(prediction>0)\n",
    "            print(\"accuracy\",accuracy(prediction,y))\n",
    "    return w1,b1,w2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "db9cb27d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(42000,)\n",
      "[8 0 6 ... 8 8 8]\n",
      "accuracy 0.12128571428571429\n",
      "50\n",
      "(42000,)\n",
      "[6 0 6 ... 6 6 6]\n",
      "accuracy 0.2328095238095238\n",
      "100\n",
      "(42000,)\n",
      "[6 6 6 ... 6 6 7]\n",
      "accuracy 0.3360714285714286\n",
      "150\n",
      "(42000,)\n",
      "[6 6 2 ... 8 6 7]\n",
      "accuracy 0.3904047619047619\n",
      "200\n",
      "(42000,)\n",
      "[6 6 2 ... 8 6 7]\n",
      "accuracy 0.4191190476190476\n",
      "250\n",
      "(42000,)\n",
      "[6 2 1 ... 8 6 7]\n",
      "accuracy 0.49766666666666665\n",
      "300\n",
      "(42000,)\n",
      "[4 2 1 ... 3 6 4]\n",
      "accuracy 0.5477857142857143\n",
      "350\n",
      "(42000,)\n",
      "[4 6 1 ... 3 6 4]\n",
      "accuracy 0.5817619047619048\n",
      "400\n",
      "(42000,)\n",
      "[4 6 7 ... 3 6 4]\n",
      "accuracy 0.5970238095238095\n",
      "450\n",
      "(42000,)\n",
      "[4 6 7 ... 3 6 4]\n",
      "accuracy 0.6141666666666666\n",
      "500\n",
      "(42000,)\n",
      "[4 2 7 ... 3 6 4]\n",
      "accuracy 0.5920714285714286\n",
      "550\n",
      "(42000,)\n",
      "[9 6 7 ... 3 6 9]\n",
      "accuracy 0.6516428571428572\n",
      "600\n",
      "(42000,)\n",
      "[9 6 7 ... 3 6 4]\n",
      "accuracy 0.6809285714285714\n",
      "650\n",
      "(42000,)\n",
      "[9 6 7 ... 3 6 4]\n",
      "accuracy 0.697\n",
      "700\n",
      "(42000,)\n",
      "[9 2 7 ... 3 2 4]\n",
      "accuracy 0.5168095238095238\n",
      "750\n",
      "(42000,)\n",
      "[9 6 7 ... 3 6 4]\n",
      "accuracy 0.7122380952380952\n"
     ]
    }
   ],
   "source": [
    "w1,b1,w2,b2 = LEARN(X,labels,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7a861d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w1,b1,w2,b2,x):\n",
    "    _,_,_, a2 = forwardprop(w1,b1,w2,b2, x)\n",
    "    output = np.argmax(a2,axis=0)\n",
    "    return output\n",
    "\n",
    "def test_prediction(ind, w1,b1,w2,b2):\n",
    "    global X,labels\n",
    "    image = X[:,ind]\n",
    "    pred = predict(w1,b1,w2,b2, X[:,ind])\n",
    "    label = labels[ind]\n",
    "    print('Label:', label)\n",
    "    print('Prediction ', pred)\n",
    "    \n",
    "    image = image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(image, interpolation='nearest')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "679b597a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# test_prediction(0,w1,b1,w2,b2)\n",
    "print([i for i in predict(w1,b1,w2,b2,X) if i>0])\n",
    "\n",
    "#so it hits the same accuracy (0.983) because it predicts them all to be zeros, and 0 happens\n",
    "#to come up that many times \n",
    "\n",
    "#/var/folders/88/k7cqn0rn6tq41vl_nhn7qdfw0000gn/T/ipykernel_78736/2736214406.py:12: RuntimeWarning: overflow encountered in exp\n",
    "  #return np.exp(a) / (np.sum(np.exp(a)))\n",
    "#/var/folders/88/k7cqn0rn6tq41vl_nhn7qdfw0000gn/T/ipykernel_78736/2736214406.py:12: RuntimeWarning: invalid value encountered in divide\n",
    "  #return np.exp(a) / (np.sum(np.exp(a)))\n",
    "    \n",
    "    \n",
    "##ahhhhh it's working!! changed np.sum() to sum() in softmax!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c1ac8c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n",
      "(10, 784)\n",
      "(10, 42000)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# w1 = np.random.normal(size = (10, 784))\n",
    "# print(X.shape)\n",
    "# print(w1.shape)\n",
    "# b1 = np.random.normal(size = (10, 1))\n",
    "# print((w1.dot(X)).shape)\n",
    "# print(b1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "654420b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3\n",
      "Prediction  [4 6 6 6 4 6 6 6 6 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(X[:,0])\n",
    "\n",
    "test_prediction(112,w1,b1,w2,b2)\n",
    "\n",
    "###very strange - why does it not give only 1 value? now I don't know what the accuracy is even checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519caf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
