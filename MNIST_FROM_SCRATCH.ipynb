{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a307bc31",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "\n",
    "KARL MAKE SURE TO UPDATE AND REFERENCE \n",
    "\n",
    "### Goal:\n",
    "An MNIST image is 784 pixels when unravelled. It is B/W, with values from 0-255. I want to create a ML model which will recognise the number written in the image.\n",
    "\n",
    "### To Investigate:\n",
    "How changing the number of hidden layer nodes affects the output. \n",
    "\n",
    "How adding layers or changing the nonlinear relationship (activation fn) between the layers affects the output. tanh, sigmoid, relu\n",
    "\n",
    "Graphing how quickly the model learns and reaches a threshold. \n",
    "\n",
    "How to squeeze a very accurate model out of it, maybe combine them? \n",
    "\n",
    "something with the back prop?\n",
    "\n",
    "how does the alpha affect it? \n",
    "\n",
    "instead of iterations, until an accuracy is reached? \n",
    "\n",
    "should I add some randomness to try find a global min?\n",
    "\n",
    "trying a different descent - with momentum? \n",
    "\n",
    "trying He initialization (typically used for ReLU activation functions) or Xavier\n",
    "\n",
    "Cross entropy loss?\n",
    "\n",
    "### How it works\n",
    "Need to transpose first for Matrix Multiplication\n",
    "#### Forward Propagation\n",
    "This is how an image will proceed through the network\n",
    "\n",
    "A[0] = input layer (784 nodes)\n",
    "\n",
    "A[1] = ReLU( w[1]*A[0]+b[1] ), ReLU is the activation fn, it makes it a nonlinear relationship, RELU is f(x) = x, x>0, =0 x<=0. w is weights and b is biases. (10 nodes)\n",
    "\n",
    "A[2] = g(w[2]*A[1] +b[2]), g is softmax as this goes to the output layer. softmax is essentially exp() / sum(exp()). gives a probability. (10 nodes, 0-1 probability of being the value 0-9)\n",
    "\n",
    "#### Backward Propagation\n",
    "The thing that makes it learn. Want to see how wrong the model was and adjust the weights and biases\n",
    "\n",
    "what I want to know is how we know how much to change the weights by? \n",
    "\n",
    "this is the part I least understand\n",
    "\n",
    "obviously you have to do it in reverse - so the transpose of the matrix and the differential of the activation fn. \n",
    "\n",
    "dz[2] = A[2] - Y, Y is the answer from the catalogue\n",
    "\n",
    "unsure about this...\n",
    "\n",
    "dw[2] = 1/m dz[2]* a[1]^T, derivative of the loss fn wrt the weights, m is no of columns (ie number of images as it's been transposed)\n",
    "\n",
    "db[2] = 1/m sum dz[2], average of the absolute error \n",
    "\n",
    "dz[1] = w[2]^T * dz[2] * g'(z[1]), 1st derivative of activation fn \n",
    "\n",
    "dw[1] = 1/m dz[1] * A[0]^T\n",
    "\n",
    "db[1] = 1/m sum(dz[1])\n",
    "\n",
    "#### Update parameters\n",
    "need to choose \\alpha, learning rate hyperparameterm\n",
    "\n",
    "for each parameter (eg w[2]) you remove the delta times learning rate (-\\alpha* dw[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a60f65",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "#### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fab11c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "#get data\n",
    "data = pd.read_csv('train.csv')\n",
    "data = np.array(data) #convert to numpy array \n",
    "\n",
    "m,n = data.shape #rows and columns m&n\n",
    "np.random.shuffle(data) #shuffle\n",
    "\n",
    "data = data.T #transpose \n",
    "\n",
    "labels = data[0] \n",
    "X = data[1:]/255 #these will be the imputs, /255 to normalise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d33775",
   "metadata": {},
   "source": [
    "data is now nxm, ie 785 x 42000 array. the extra row is the label of 0-9\n",
    "\n",
    "#### Generate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f45c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RELU(a):\n",
    "    \"\"\"\n",
    "    Performs the ReLU function, if the value is above zero it is returned, if it is below zero, zero is returned\n",
    "    \"\"\"\n",
    "    return np.maximum(a,0)\n",
    "\n",
    "def softmax(a):\n",
    "    \"\"\"\n",
    "    Performs softmax, essentially will get a probability out\n",
    "    \"\"\"\n",
    "    ####Not sure about this one \n",
    "#     return np.exp(a) / (np.sum(np.exp(a),axis=0)) ###changed back to np.sum\n",
    "    a = a - np.max(a, axis=0, keepdims=True)  # Stability improvement\n",
    "    return np.exp(a) / np.sum(np.exp(a), axis=0, keepdims=True)\n",
    "\n",
    "def RELU_prime(a):\n",
    "    \"\"\"\n",
    "    The derivative of the RELU fn. Returns 1 if x>0, 0 if x<=0\n",
    "    \"\"\"\n",
    "  \n",
    "    \n",
    "    return a>0 \n",
    "\n",
    "\n",
    "\n",
    "def initialise():\n",
    "    \"\"\"\n",
    "    Randomly initialises the weights and biases using a normal distribution\n",
    "    \"\"\"\n",
    "    w1 = np.random.normal(size = (10, 784))\n",
    "    b1 = np.random.normal(size = (10, 1))\n",
    "    w2 = np.random.normal(size = (10, 10))\n",
    "    b2 = np.random.normal(size = (10, 1))\n",
    "    \n",
    "    return w1,b1,w2,b2\n",
    "\n",
    "\n",
    "def forwardprop(w1,b1, w2,b2, x):\n",
    "    \"\"\"\n",
    "    Forward propagates the image through the net\n",
    "    \"\"\"\n",
    "    z1 = w1.dot(x) + b1 #apply weights and biases\n",
    "    a1 = RELU(z1) #activate layer\n",
    "    \n",
    "    z2 = w2.dot(a1) + b2 #apply w&b for 2nd layer\n",
    "    a2 = softmax(z2) #activate for output layer\n",
    "    \n",
    "    return z1,a1,z2,a2\n",
    "\n",
    "def fix(y):\n",
    "    #called one-hotting? (dk what that means)\n",
    "    Y = np.zeros((y.size, y.max() + 1))\n",
    "    Y[np.arange(y.size), y] = 1\n",
    "    return Y.T\n",
    "\n",
    "def backprop(z1,a1, z2,a2, w1,w2, x,y): #y is labels \n",
    "    global m \n",
    "    #need to get y in a nice format before putting it in here\n",
    "    y = fix(y)\n",
    "    \n",
    "    dz2 = a2 - y\n",
    "    dw2 = 1/m * dz2.dot(a1.T)\n",
    "    db2 = 1/m * np.sum(dz2,1,keepdims=True)\n",
    "#     print('backpropworkings: dz2,db2')\n",
    "#     print(dz2.shape)\n",
    "# #     print(dw2.shape)\n",
    "#     print(db2.shape)\n",
    "    \n",
    "#     print(dz2)\n",
    "#     print(dw2)\n",
    "#     print(db2)\n",
    "#     print('np.sum')\n",
    "#     print(np.sum(dz2))\n",
    "#     print('1/m * np.sum')\n",
    "#     print(1/m * np.sum(dz2,1))\n",
    "#     print('sum')\n",
    "#     print(sum(dz2).shape)\n",
    "    \n",
    "    \n",
    "    dz1 = (w2.T).dot(dz2) * RELU_prime(z1)\n",
    "    dw1 = 1/m * dz1.dot(x.T)\n",
    "    db1 = 1/m * np.sum(dz1,1, keepdims=True)\n",
    "    \n",
    "    return dw1, db1,dw2,db2\n",
    "\n",
    "def update(w1,b1, w2,b2, dw1,db1, dw2,db2, alpha):\n",
    "    w1 = w1 - alpha * dw1\n",
    "    b1 = b1 - alpha * db1\n",
    "    w2 = w2 - alpha * dw2\n",
    "    b2 = b2 - alpha * db2\n",
    "    return w1,b1, w2,b2\n",
    "        \n",
    "def accuracy(prediction,y):\n",
    "    if y.ndim > 1:  #Make sure it's not one hotted\n",
    "        y = np.argmax(y, axis=0)\n",
    "    return np.sum(prediction==y)/y.size\n",
    "\n",
    "# def accuracy(prediction,y):\n",
    "#     for Y in range(len(y)):\n",
    "# #         if Y.ndim > 1:  #Make sure it's not one hotted\n",
    "# #             Y = np.argmax(y, axis=0)\n",
    "#         return np.sum(prediction==Y)/y.size\n",
    "##NEVERMIND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82aa7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LEARN(x,y, iterations, alpha=0.7):\n",
    "    w1,b1,w2,b2 = initialise()\n",
    "#     print('init w1b1')\n",
    "#     print(w1.shape,b1.shape,w2.shape,b2.shape)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        z1,a1,z2,a2 = forwardprop(w1,b1,w2,b2, x)\n",
    "#         print('forward, z1,a1,z2,a2')\n",
    "#         print(z1.shape,a1.shape,z2.shape,a2.shape)\n",
    "        \n",
    "        dw1,db1, dw2,db2 = backprop(z1,a1, z2,a2, w1,w2, x,y)\n",
    "#         print('backward,dw1,db1, dw2,db2')\n",
    "#         print(dw1.shape,db1.shape, dw2.shape,db2.shape)\n",
    "        \n",
    "        w1,b1, w2,b2 = update(w1,b1, w2,b2, dw1,db1, dw2,db2, alpha)\n",
    "        \n",
    "        if i % 50 ==0:\n",
    "            print(i)\n",
    "#             print('a2', a2.shape)\n",
    "            prediction = np.argmax(a2,axis=0)\n",
    "            print(prediction.shape)\n",
    "            print(prediction)\n",
    "#             print(\"pred\",prediction.shape)\n",
    "#             print(prediction>0)\n",
    "            print(\"accuracy\",accuracy(prediction,y))\n",
    "    return w1,b1,w2,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dce5dbd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(42000,)\n",
      "[9 4 9 ... 8 0 9]\n",
      "accuracy 0.10902380952380952\n",
      "50\n",
      "(42000,)\n",
      "[5 3 0 ... 1 0 9]\n",
      "accuracy 0.13464285714285715\n",
      "100\n",
      "(42000,)\n",
      "[5 3 5 ... 1 5 9]\n",
      "accuracy 0.07588095238095238\n",
      "150\n",
      "(42000,)\n",
      "[4 3 2 ... 1 2 9]\n",
      "accuracy 0.04488095238095238\n",
      "200\n",
      "(42000,)\n",
      "[9 7 5 ... 1 5 9]\n",
      "accuracy 0.040023809523809524\n",
      "250\n",
      "(42000,)\n",
      "[9 7 5 ... 1 5 9]\n",
      "accuracy 0.08271428571428571\n",
      "300\n",
      "(42000,)\n",
      "[9 7 5 ... 1 5 9]\n",
      "accuracy 0.07997619047619048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m w1,b1,w2,b2 \u001b[38;5;241m=\u001b[39m LEARN(X,labels,\u001b[38;5;241m800\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mLEARN\u001b[0;34m(x, y, iterations, alpha)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     print('init w1b1')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     print(w1.shape,b1.shape,w2.shape,b2.shape)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m----> 7\u001b[0m         z1,a1,z2,a2 \u001b[38;5;241m=\u001b[39m forwardprop(w1,b1,w2,b2, x)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#         print('forward, z1,a1,z2,a2')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#         print(z1.shape,a1.shape,z2.shape,a2.shape)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         dw1,db1, dw2,db2 \u001b[38;5;241m=\u001b[39m backprop(z1,a1, z2,a2, w1,w2, x,y)\n",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m, in \u001b[0;36mforwardprop\u001b[0;34m(w1, b1, w2, b2, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforwardprop\u001b[39m(w1,b1, w2,b2, x):\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    Forward propagates the image through the net\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m w1\u001b[38;5;241m.\u001b[39mdot(x) \u001b[38;5;241m+\u001b[39m b1 \u001b[38;5;66;03m#apply weights and biases\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m RELU(z1) \u001b[38;5;66;03m#activate layer\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m w2\u001b[38;5;241m.\u001b[39mdot(a1) \u001b[38;5;241m+\u001b[39m b2 \u001b[38;5;66;03m#apply w&b for 2nd layer\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w1,b1,w2,b2 = LEARN(X,labels,800)#800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53623b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w1,b1,w2,b2,x):\n",
    "    #must get a single image\n",
    "    _,_,_, a2 = forwardprop(w1,b1,w2,b2, x)\n",
    "    output = np.argmax(a2,axis=0)\n",
    "    return output[0]\n",
    "\n",
    "def test_prediction(ind, w1,b1,w2,b2):\n",
    "    global X,labels\n",
    "    image = X[:,ind]\n",
    "    pred = predict(w1,b1,w2,b2, image)\n",
    "    label = labels[ind]\n",
    "    print('Label:', label)\n",
    "    print('Prediction ', pred)\n",
    "    print(label == pred)\n",
    "    \n",
    "    image = image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(image, interpolation='nearest')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "021c5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prediction(0,w1,b1,w2,b2)\n",
    "# print([i for i in predict(w1,b1,w2,b2,X) if i>0])\n",
    "\n",
    "#so it hits the same accuracy (0.983) because it predicts them all to be zeros, and 0 happens\n",
    "#to come up that many times \n",
    "\n",
    "#/var/folders/88/k7cqn0rn6tq41vl_nhn7qdfw0000gn/T/ipykernel_78736/2736214406.py:12: RuntimeWarning: overflow encountered in exp\n",
    "  #return np.exp(a) / (np.sum(np.exp(a)))\n",
    "#/var/folders/88/k7cqn0rn6tq41vl_nhn7qdfw0000gn/T/ipykernel_78736/2736214406.py:12: RuntimeWarning: invalid value encountered in divide\n",
    "  #return np.exp(a) / (np.sum(np.exp(a)))\n",
    "    \n",
    "    \n",
    "##ahhhhh it's working!! changed np.sum() to sum() in softmax!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e251a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1 = np.random.normal(size = (10, 784))\n",
    "# print(X.shape)\n",
    "# print(w1.shape)\n",
    "# b1 = np.random.normal(size = (10, 1))\n",
    "# print((w1.dot(X)).shape)\n",
    "# print(b1.shape)\n",
    "# assert X.shape == (784, m), \"Input shape mismatch\"\n",
    "# assert w1.shape == (10, 784), \"w1 shape mismatch\"\n",
    "# assert labels.max() <= 9, \"Labels out of range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87bed689",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(X[:,0])\n",
    "\n",
    "for i in range(0,14):\n",
    "    index = np.random.randint(0,42000)\n",
    "    \n",
    "    print(index)\n",
    "    test_prediction(index,w1,b1,w2,b2)\n",
    "#     print(index == test_prediction(index,w1,b1,w2,b2))\n",
    "    \n",
    "# #     test_prediction(i,w1,b1,w2,b2)\n",
    "# test_prediction(1000,w1,b1,w2,b2)\n",
    "\n",
    "###very strange - why does it not give only 1 value? now I don't know what the accuracy is even checking \n",
    "np.random.randint(5000,50000)\n",
    "\n",
    "\n",
    "##### NEW ISSUE: \n",
    "#it is just not as accurate as it should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f75979f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1\n",
    "#issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f550767",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
